{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Building a Vision Transformer (ViT) from Scratch\n",
    "\n",
    "In this tutorial, you'll implement each component of the Vision Transformer step by step. Each module is provided as a stub with descriptions to guide your implementation."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Setup (Just run)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "from pathlib import Path\n",
    "from dataclasses import dataclass\n",
    "import pytest\n",
    "import torch as t\n",
    "import torch.nn as nn\n",
    "from torch import Tensor\n",
    "from jaxtyping import Float\n",
    "import functools\n",
    "import nanoclip.vit as nanoclip_vit\n",
    "\n",
    "\n",
    "def replace_nanoclip_implementation(cls):\n",
    "    @functools.wraps(cls)\n",
    "    def wrapper(*args, **kwargs):\n",
    "        return cls(*args, **kwargs)\n",
    "\n",
    "    setattr(nanoclip_vit, cls.__name__, cls)\n",
    "    return wrapper\n",
    "\n",
    "def remove_test_module(module_name):\n",
    "    modules_to_remove = [name for name in sys.modules if name.startswith(module_name)]\n",
    "    for module_name in modules_to_remove:\n",
    "        del sys.modules[module_name]\n",
    "\n",
    "\n",
    "def run_test(test_name, test_func):\n",
    "    remove_test_module(\"test_vit_clip\")\n",
    "    project_root = Path.cwd().resolve().parent\n",
    "    setattr(nanoclip_vit, test_func.__name__, test_func)\n",
    "    pytest.main([\"-v\", \"-p\", \"no:cacheprovider\", \"-s\", f\"{project_root}/tests/test_vit_clip.py::{test_name}\"])\n",
    "\n",
    "\n",
    "# Bit of a hack, ViTConfig doesn't seem to be able to be overwritted\n",
    "def test_vit_config(vit_config_cls):\n",
    "    config = vit_config_cls(\n",
    "        n_layers=12,\n",
    "        d_model=768,\n",
    "        d_proj=512,\n",
    "        image_res=(224, 224),\n",
    "        patch_size=(16, 16),\n",
    "        n_heads=12,\n",
    "        norm_data=((0.5, 0.5, 0.5), (0.5, 0.5, 0.5)),\n",
    "    )\n",
    "    assert config.num_patches == (14, 14)\n",
    "    assert config.seq_length == 197  # 14 * 14 + 1 (CLS token)\n",
    "    assert config.d_head == 64  # 768 // 12"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. ViT Configuration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "@dataclass\n",
    "class ViTConfig:\n",
    "    n_layers: int\n",
    "    d_model: int\n",
    "    d_proj: int\n",
    "    image_res: tuple[int, int]\n",
    "    patch_size: tuple[int, int]\n",
    "    n_heads: int\n",
    "    norm_data: tuple[\n",
    "        tuple[float, float, float], tuple[float, float, float]\n",
    "    ]  # (mean, std)\n",
    "\n",
    "    mlp_mult: int = 4\n",
    "    causal_attn: bool = False\n",
    "\n",
    "    # Calculated in __post_init__\n",
    "    d_head: int = None  # type: ignore\n",
    "    num_patches: tuple[int, int] = None  # type: ignore\n",
    "    seq_length: int = None  # type: ignore\n",
    "\n",
    "    def __post_init__(self):\n",
    "        \"\"\"\n",
    "        Compute the attributes based on initial config:\n",
    "\n",
    "        1. num_patches: Number of patches in based on the height & width and patch_size.\n",
    "        2. seq_length: number_of_patches + the CLS token\n",
    "        3. d_head: Dimension of each head. Use d_model & n_heads\n",
    "        \"\"\"\n",
    "        raise NotImplementedError()\n",
    "\n",
    "# run_test(\"test_vit_config\", ViTConfig)\n",
    "test_vit_config(ViTConfig)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Patch Embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class PatchEmbedding(nn.Module):\n",
    "    def __init__(self, cfg: ViTConfig):\n",
    "        super().__init__()\n",
    "        self.cfg = cfg\n",
    "        self.class_embedding = nn.Parameter(t.empty(cfg.d_model))\n",
    "        self.patch_embedding = nn.Parameter(\n",
    "            t.empty(cfg.d_model, 3 * cfg.patch_size[0] * cfg.patch_size[1])\n",
    "        )\n",
    "        self.position_embedding = nn.Parameter(t.empty(cfg.seq_length, cfg.d_model))\n",
    "\n",
    "    def forward(\n",
    "        self, pixel_values: Float[Tensor, \"batch 3 height width\"]\n",
    "    ) -> Float[Tensor, \"batch seq d_model\"]:\n",
    "        \"\"\"\n",
    "        Forward pass for PatchEmbeddings:\n",
    "        1. Rearrange pixel_values into a sequence of patches:\n",
    "          - Hint: Should be of shape (batch, num_patches, channels * patch_size)\n",
    "        2. Project the patches with patch_embedding\n",
    "        3. Add the class embedding\n",
    "        4. Add the position embeddings\n",
    "        \"\"\"\n",
    "        # Implementation steps here (refer to lines 70-96 in nanoclip/vit.py)\n",
    "        raise NotImplementedError()\n",
    "\n",
    "run_test(\"test_patch_embedding\", PatchEmbedding)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Attention\n",
    "\n",
    "ViTs use **full attention**, you _should not_ use a causal mask.\n",
    "\n",
    "Many different ways of implementing attention -- I tend to like a very explicit einops operation.\n",
    "\n",
    "#### Q: Why would this be the case\n",
    "\n",
    "> Answer: We want each patch to fully attend to all other patches. The final output is the (first) CLS token.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Attention(nn.Module):\n",
    "    def __init__(self, cfg: ViTConfig):\n",
    "        \"\"\"\n",
    "        Initialize the Attention module:\n",
    "        1. Create query, key, and value projection layers\n",
    "        2. Create output projection layer\n",
    "        \"\"\"\n",
    "        super().__init__()\n",
    "        self.cfg = cfg\n",
    "        self.q_proj = nn.Linear(cfg.d_model, cfg.d_model, bias=True)\n",
    "        self.k_proj = nn.Linear(cfg.d_model, cfg.d_model, bias=True)\n",
    "        self.v_proj = nn.Linear(cfg.d_model, cfg.d_model, bias=True)\n",
    "        self.out_proj = nn.Linear(cfg.d_model, cfg.d_model, bias=True)\n",
    "\n",
    "    def forward(\n",
    "        self, x: Float[Tensor, \"batch seq d_model\"]\n",
    "    ) -> Float[Tensor, \"batch seq d_model\"]:\n",
    "        \"\"\"\n",
    "        Forward pass for Attention:\n",
    "        1. Project input to query, key, and value\n",
    "        2. Reshape q, k, v to separate the heads\n",
    "        3. Compute (full) attention scores\n",
    "        4. Apply softmax to get attention weights\n",
    "        5. Apply attention weights to values\n",
    "        6. Reshape and project to output\n",
    "        \"\"\"\n",
    "        # (Solution: 109-141 nanoclip/vit.py)\n",
    "        raise NotImplementedError()\n",
    "\n",
    "run_test(\"test_attn\", Attention)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. MLP"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MLP(nn.Module):\n",
    "    def __init__(self, cfg: ViTConfig):\n",
    "        \"\"\"\n",
    "        Initialize the MLP module:\n",
    "        1. Create up-projection layer\n",
    "        2. Create down-projection layer\n",
    "        \"\"\"\n",
    "        super().__init__()\n",
    "        self.cfg = cfg\n",
    "        self.up_proj = nn.Linear(cfg.d_model, cfg.d_model * cfg.mlp_mult, bias=True)\n",
    "        self.down_proj = nn.Linear(cfg.d_model * cfg.mlp_mult, cfg.d_model, bias=True)\n",
    "\n",
    "    def forward(\n",
    "        self, x: Float[Tensor, \"batch seq d_model\"]\n",
    "    ) -> Float[Tensor, \"batch seq d_model\"]:\n",
    "        \"\"\"\n",
    "        Forward pass for MLP:\n",
    "        1. Apply up-projection\n",
    "        2. Apply activation function (quick_gelu)\n",
    "        3. Apply down-projection\n",
    "        \"\"\"\n",
    "        # Implementation steps here (refer to lines 151-155 in nanoclip/vit.py)\n",
    "        raise NotImplementedError()\n",
    "\n",
    "run_test(\"test_mlp\", MLP)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Transformer Block\n",
    "\n",
    "Now we're putting it all together."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "@replace_nanoclip_implementation\n",
    "class TransformerBlock(nn.Module):\n",
    "    def __init__(self, cfg: ViTConfig):\n",
    "        super().__init__()\n",
    "        self.cfg = cfg\n",
    "        self.attn = Attention(cfg)\n",
    "        self.ln1 = nn.LayerNorm(cfg.d_model)\n",
    "        self.mlp = MLP(cfg)\n",
    "        self.ln2 = nn.LayerNorm(cfg.d_model)\n",
    "\n",
    "    def forward(\n",
    "        self, x: Float[Tensor, \"batch seq d_model\"]\n",
    "    ) -> Float[Tensor, \"batch seq d_model\"]:\n",
    "        \"\"\"\n",
    "        1. Apply layer norm -> attention to the input, add to residual\n",
    "        2. Apply layer norm -> MLP, add to residual\n",
    "        \"\"\"\n",
    "        # Implementation steps here (refer to lines 167-170 in nanoclip/vit.py)\n",
    "        raise NotImplementedError()\n",
    "\n",
    "run_test(\"test_xfmer_block\", TransformerBlock)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Assembling the Vision Transformer (ViT) Model\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "@replace_nanoclip_implementation\n",
    "class ViT(nn.Module):\n",
    "    def __init__(self, cfg: ViTConfig):\n",
    "        super().__init__()\n",
    "        self.cfg = cfg\n",
    "        self.embed = PatchEmbedding(cfg)\n",
    "        self.pre_ln = nn.LayerNorm(cfg.d_model)\n",
    "        self.blocks = nn.ModuleList(\n",
    "            [TransformerBlock(cfg) for _ in range(cfg.n_layers)]\n",
    "        )\n",
    "        self.post_ln = nn.LayerNorm(cfg.d_model)\n",
    "        self.out_proj = nn.Linear(cfg.d_model, cfg.d_proj, bias=False)\n",
    "\n",
    "    def forward(\n",
    "        self, pixel_values: Float[Tensor, \"batch 3 height width\"]\n",
    "    ) -> Float[Tensor, \"batch d_proj\"]:\n",
    "        \"\"\"\n",
    "        1. Apply patch embeddings\n",
    "        2. Apply pre-layer normalization\n",
    "        3. Apply transformer blocks\n",
    "        4. Select CLS token\n",
    "        5. Apply post-layer normalization\n",
    "        6. Apply output projection\n",
    "        \"\"\"\n",
    "        # Implementation steps here (refer to lines 183-192 in nanoclip/vit.py)\n",
    "        raise NotImplementedError()\n",
    "\n",
    "run_test(\"test_e2e\", ViT)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Conclusion\n",
    "\n",
    "You've set up the structure for implementing each component of the Vision Transformer. Complete each method by replacing the `NotImplementedError` with the appropriate code as guided by the docstrings. Once all components are implemented, you can proceed to integrate and test the ViT model."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
